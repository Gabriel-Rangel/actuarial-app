{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05476df0-f786-4202-8ebe-2c2e500320ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criando Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a467ff1-53c5-45b2-9767-abdc27d98f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "catalog_name = f\"actuarial_app_catalog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"GRANT CREATE CATALOG ON METASTORE TO `account users`\")\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.variable_annuity\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.long_term_care\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b80f9d7c-8154-4ac8-a298-412b4c487624",
     "showTitle": true,
     "title": "ALTERE ESSE PARAMETRO"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "\n",
    "folders = {'va': 'variable_annuity', 'ltc': 'long_term_care'}\n",
    "\n",
    "for folder, schema in folders.items():\n",
    "\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "\n",
    "            entity_name = file[:-4]\n",
    "\n",
    "            table_name = f\"{catalog_name}.{schema}.{entity_name}\"\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            s_df = spark.createDataFrame(df)\n",
    "\n",
    "            s_df.write.mode(\"overwrite\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "# Apply constraints to ltc_incidence_w_comment\n",
    "# Note: Primary Key columns must be NOT NULL in Delta tables\n",
    "\n",
    "table_name = f\"{catalog_name}.long_term_care.ltc_incidence_w_comment\"\n",
    "pk_cols = [\n",
    "    \"Group_Indicator\", \"Gender\", \"Issue_Age_Bucket\", \"Incurred_Age_Bucket\",\n",
    "    \"Issue_Year_Bucket\", \"Policy_Year\", \"Marital_Status\", \"Premium_Class\",\n",
    "    \"Underwriting_Type\", \"Coverage_Type_Bucket\", \"Tax_Qualification_Status\",\n",
    "    \"Inflation_Rider\", \"Rate_Increase_Flag\", \"Restoration_of_Benefits\",\n",
    "    \"NH_Orig_Daily_Ben_Bucket\", \"ALF_Orig_Daily_Ben_Bucket\",\n",
    "    \"HHC_Orig_Daily_Ben_Bucket\", \"NH_Ben_Period_Bucket\",\n",
    "    \"ALF_Ben_Period_Bucket\", \"HHC_Ben_Period_Bucket\", \"NH_EP_Bucket\",\n",
    "    \"ALF_EP_Bucket\", \"HHC_EP_Bucket\", \"Region\"\n",
    "]\n",
    "\n",
    "print(f\"Applying constraints to {table_name}...\")\n",
    "\n",
    "# 1. Set columns to NOT NULL\n",
    "for col in pk_cols:\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {table_name} ALTER COLUMN {col} SET NOT NULL\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not set NOT NULL on {col} (might already be set): {e}\")\n",
    "\n",
    "# 2. Add Primary Key Constraint\n",
    "try:\n",
    "    # Check if constraint already exists to avoid error\n",
    "    # (Simple check via try/except is often easiest for scripts)\n",
    "    spark.sql(f\"ALTER TABLE {table_name} ADD CONSTRAINT ltc_incidence_w_comment_pk PRIMARY KEY ({', '.join(pk_cols)})\")\n",
    "    print(f\"Successfully added PK constraint to {table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not add PK constraint (might already exist): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "# Apply constraints to ltc_termination_w_comment\n",
    "# Note: Primary Key columns must be NOT NULL in Delta tables\n",
    "\n",
    "table_name_term = f\"{catalog_name}.long_term_care.ltc_termination_w_comment\"\n",
    "pk_cols_term = [\n",
    "    \"Gender\", \"Incurred_Age_Bucket\", \"Region\", \"Incurred_Year_Bucket\",\n",
    "    \"Claim_Type\", \"Diagnosis_Category\", \"Claim_Duration\"\n",
    "]\n",
    "\n",
    "print(f\"Applying constraints to {table_name_term}...\")\n",
    "\n",
    "# 1. Set columns to NOT NULL\n",
    "for col in pk_cols_term:\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {table_name_term} ALTER COLUMN {col} SET NOT NULL\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not set NOT NULL on {col} (might already be set): {e}\")\n",
    "\n",
    "# 2. Add Primary Key Constraint\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {table_name_term} ADD CONSTRAINT ltc_termination_w_comment_pk PRIMARY KEY ({', '.join(pk_cols_term)})\")\n",
    "    print(f\"Successfully added PK constraint to {table_name_term}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not add PK constraint (might already exist): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Dashboards and Genie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Databricks context\n",
    "try:\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    host_name = ctx.apiUrl().get()\n",
    "    token = ctx.apiToken().get()\n",
    "except:\n",
    "    # Fallback for local development if env vars are set\n",
    "    host_name = os.environ.get(\"DATABRICKS_HOST\")\n",
    "    token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "if not host_name or not token:\n",
    "    raise Exception(\"Databricks host and token must be available.\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# Helper function to get a warehouse ID\n",
    "def get_warehouse_id():\n",
    "    url = f\"{host_name}/api/2.0/sql/warehouses\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        warehouses = response.json().get(\"warehouses\", [])\n",
    "        for wh in warehouses:\n",
    "            if wh.get(\"state\") == \"RUNNING\":\n",
    "                return wh.get(\"id\")\n",
    "        if warehouses:\n",
    "            return warehouses[0].get(\"id\")\n",
    "    return None\n",
    "\n",
    "warehouse_id = get_warehouse_id()\n",
    "print(f\"Using Warehouse ID: {warehouse_id}\")\n",
    "\n",
    "# --- Dashboards ---\n",
    "dashboard_files = [\n",
    "    \"dashboards/LTC experience study - Incidence.lvdash.json\",\n",
    "    \"dashboards/Variable Annuities Projections.lvdash.json\"\n",
    "]\n",
    "\n",
    "# Map display names to env var placeholders in app.yaml\n",
    "dashboard_env_map = {\n",
    "    \"LTC experience study - Incidence\": \"DATABRICKS_DASHBOARD_LTC_ID\",\n",
    "    \"Variable Annuities Projections\": \"DATABRICKS_DASHBOARD_VA_ID\"\n",
    "}\n",
    "\n",
    "collected_ids = {}\n",
    "\n",
    "# Fetch existing dashboards to avoid duplicates\n",
    "existing_dashboards = {}\n",
    "list_dashboards_url = f\"{host_name}/api/2.0/lakeview/dashboards\"\n",
    "next_page_token = None\n",
    "\n",
    "print(\"Listing existing dashboards...\")\n",
    "while True:\n",
    "    params = {}\n",
    "    if next_page_token:\n",
    "        params['page_token'] = next_page_token\n",
    "        \n",
    "    list_response = requests.get(list_dashboards_url, headers=headers, params=params)\n",
    "    if list_response.status_code == 200:\n",
    "        data = list_response.json()\n",
    "        dashboards = data.get(\"dashboards\", [])\n",
    "        for db in dashboards:\n",
    "            existing_dashboards[db.get(\"display_name\")] = db.get(\"dashboard_id\")\n",
    "        \n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Warning: Could not list existing dashboards: {list_response.text}\")\n",
    "        break\n",
    "\n",
    "print(f\"Found {len(existing_dashboards)} existing dashboards.\")\n",
    "\n",
    "for file_path in dashboard_files:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "        \n",
    "    with open(file_path, 'r') as f:\n",
    "        dashboard_content = f.read()\n",
    "        \n",
    "    # The file name without extension is the display name\n",
    "    display_name = os.path.basename(file_path).replace(\".lvdash.json\", \"\")\n",
    "    \n",
    "    dashboard_id = None\n",
    "    \n",
    "    # Check if dashboard already exists\n",
    "    if display_name in existing_dashboards:\n",
    "        dashboard_id = existing_dashboards[display_name]\n",
    "        print(f\"Dashboard '{display_name}' already exists with ID: {dashboard_id}. Skipping creation.\")\n",
    "    else:\n",
    "        # Create Dashboard\n",
    "        create_url = f\"{host_name}/api/2.0/lakeview/dashboards\"\n",
    "        payload = {\n",
    "            \"display_name\": display_name,\n",
    "            \"serialized_dashboard\": dashboard_content\n",
    "        }\n",
    "        if warehouse_id:\n",
    "            payload[\"warehouse_id\"] = warehouse_id\n",
    "            \n",
    "        response = requests.post(create_url, headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            dashboard_id = response.json().get(\"dashboard_id\")\n",
    "            print(f\"Created dashboard '{display_name}' with ID: {dashboard_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to create dashboard '{display_name}': {response.text}\")\n",
    "            continue\n",
    "\n",
    "    if dashboard_id:\n",
    "        # Store ID for app.yaml update\n",
    "        if display_name in dashboard_env_map:\n",
    "             collected_ids[dashboard_env_map[display_name]] = dashboard_id\n",
    "        \n",
    "        # Publish Dashboard\n",
    "        publish_url = f\"{host_name}/api/2.0/lakeview/dashboards/{dashboard_id}/published\"\n",
    "        publish_payload = {\n",
    "            \"embed_credentials\": True\n",
    "        }\n",
    "        if warehouse_id:\n",
    "            publish_payload[\"warehouse_id\"] = warehouse_id\n",
    "            \n",
    "        pub_response = requests.post(publish_url, headers=headers, json=publish_payload)\n",
    "        if pub_response.status_code == 200:\n",
    "            print(f\"Published dashboard '{display_name}'\")\n",
    "        else:\n",
    "            print(f\"Failed to publish dashboard '{display_name}': {pub_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Genie Spaces ---\n",
    "genie_spaces_dir = \"genie_spaces\"\n",
    "genie_spaces_config = []\n",
    "\n",
    "if os.path.exists(genie_spaces_dir):\n",
    "    for filename in os.listdir(genie_spaces_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            try:\n",
    "                with open(os.path.join(genie_spaces_dir, filename), 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                    # Join instructions if it's a list\n",
    "                    if isinstance(config.get(\"instructions\"), list):\n",
    "                        config[\"instructions\"] = \"\\n\".join(config[\"instructions\"])\n",
    "                    genie_spaces_config.append(config)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading Genie Space config {filename}: {e}\")\n",
    "else:\n",
    "    print(f\"Directory {genie_spaces_dir} not found.\")\n",
    "\n",
    "for space_config in genie_spaces_config:\n",
    "    schema = space_config[\"schema\"]\n",
    "    # Get tables in schema\n",
    "    try:\n",
    "        tables_df = spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema}\")\n",
    "        table_names = [f\"{catalog_name}.{schema}.{row.tableName}\" for row in tables_df.collect()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tables for schema {schema}: {e}\")\n",
    "        table_names = []\n",
    "\n",
    "    if not table_names:\n",
    "        print(f\"No tables found for schema {schema}, skipping Genie Space creation.\")\n",
    "        continue\n",
    "\n",
    "    # Construct inner payload (the content of the space)\n",
    "    inner_payload = {\n",
    "        \"version\": 1,\n",
    "        \"data_sources\": {\n",
    "            \"tables\": [{\"identifier\": t} for t in table_names]\n",
    "        },\n",
    "        \"instructions\": {\n",
    "            \"text_instructions\": [\n",
    "                {\n",
    "                    \"id\": str(uuid.uuid4()).replace('-', ''),\n",
    "                    \"content\": [space_config[\"instructions\"].strip()]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Construct outer payload\n",
    "    # The API expects 'serialized_space' to be a JSON string of the space configuration\n",
    "    # AND it also expects 'parent_path' to be set.\n",
    "    # We will use the current user's home folder as the parent path.\n",
    "    \n",
    "    current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    parent_path = f\"/Users/{current_user}\"\n",
    "    \n",
    "    payload = {\n",
    "        \"title\": space_config[\"name\"],\n",
    "        \"description\": space_config[\"description\"],\n",
    "        \"parent_path\": parent_path,\n",
    "        \"serialized_space\": json.dumps(inner_payload)\n",
    "    }\n",
    "    \n",
    "    if warehouse_id:\n",
    "        payload[\"warehouse_id\"] = warehouse_id\n",
    "    \n",
    "    create_url = f\"{host_name}/api/2.0/genie/spaces\"\n",
    "    response = requests.post(create_url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        space_id = response.json().get(\"space_id\")\n",
    "        print(f\"Created Genie Space '{space_config['name']}' with ID: {space_id}\")\n",
    "        collected_ids[space_config[\"env_var\"]] = space_id\n",
    "    else:\n",
    "        print(f\"Failed to create Genie Space '{space_config['name']}': {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update app.yaml\n",
    "app_yaml_path = \"app/app.yaml\"\n",
    "if os.path.exists(app_yaml_path) and collected_ids:\n",
    "    with open(app_yaml_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    for env_name, db_id in collected_ids.items():\n",
    "        # Simple replacement assuming the value is set to the env var name as a placeholder\n",
    "        # This relies on the user resetting the app.yaml to the template state before running\n",
    "        placeholder = f\"value: '{env_name}'\"\n",
    "        new_line = f\"value: '{db_id}'\"\n",
    "        \n",
    "        if placeholder in content:\n",
    "            content = content.replace(placeholder, new_line)\n",
    "            print(f\"Updated {env_name} in app.yaml\")\n",
    "        else:\n",
    "            print(f\"Could not find placeholder '{placeholder}' for {env_name} in app.yaml\")\n",
    "        \n",
    "    with open(app_yaml_path, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(\"Finished updating app.yaml\")\n",
    "else:\n",
    "    print(\"app.yaml not found or no IDs collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the App\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App Configuration\n",
    "app_name = \"actuarial-app\"\n",
    "\n",
    "# Determine source code path\n",
    "try:\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    # Assuming the notebook is at the root of the project in the workspace\n",
    "    # and the app code is in the 'app' subdirectory\n",
    "    project_root = os.path.dirname(notebook_path)\n",
    "    app_source_path = f\"{project_root}/app\"\n",
    "    print(f\"Derived app source path: {app_source_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Could not determine notebook path (likely running locally). Skipping automatic deployment.\")\n",
    "    app_source_path = None\n",
    "\n",
    "if app_source_path:\n",
    "    # 1. Check if App exists\n",
    "    get_app_url = f\"{host_name}/api/2.0/apps/{app_name}\"\n",
    "    app_response = requests.get(get_app_url, headers=headers)\n",
    "    \n",
    "    if app_response.status_code == 404:\n",
    "        # Create App\n",
    "        print(f\"App '{app_name}' not found. Creating...\")\n",
    "        create_app_url = f\"{host_name}/api/2.0/apps\"\n",
    "        create_payload = {\n",
    "            \"name\": app_name,\n",
    "            \"description\": \"Actuarial App with Embedded Dashboards and Genie\"\n",
    "        }\n",
    "        create_response = requests.post(create_app_url, headers=headers, json=create_payload)\n",
    "        if create_response.status_code == 200:\n",
    "            print(f\"Successfully created app '{app_name}'\")\n",
    "        else:\n",
    "            print(f\"Failed to create app: {create_response.text}\")\n",
    "            app_source_path = None # Stop deployment\n",
    "    elif app_response.status_code == 200:\n",
    "        print(f\"App '{app_name}' already exists.\")\n",
    "    else:\n",
    "        print(f\"Error checking app status: {app_response.text}\")\n",
    "        app_source_path = None\n",
    "\n",
    "    # 2. Deploy App\n",
    "    if app_source_path:\n",
    "        print(f\"Deploying app from {app_source_path}...\")\n",
    "        deploy_url = f\"{host_name}/api/2.0/apps/{app_name}/deployments\"\n",
    "        deploy_payload = {\n",
    "            \"source_code_path\": app_source_path\n",
    "        }\n",
    "        deploy_response = requests.post(deploy_url, headers=headers, json=deploy_payload)\n",
    "        \n",
    "        if deploy_response.status_code == 200:\n",
    "            deployment_id = deploy_response.json().get(\"deployment_id\")\n",
    "            print(f\"Deployment initiated. ID: {deployment_id}\")\n",
    "            print(f\"Track status at: {host_name}/compute/apps/{app_name}/deployments/{deployment_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to deploy app: {deploy_response.text}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 597306790085131,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "lab02_01_carga_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
